{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESA WorldCereal - Global Crop Monitoring at Field-Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Using the [ESA WorldCereal](https://www.esa.int/Applications/Observing_the_Earth/Introducing_World_Cereal) dataset, this tutorial demonstrates how to:\n",
    "- generate any number of global maize, winter cereals and spring cereals farm point locations (longitude, latitude coordinate pairs) from the .tif classification mask files\n",
    "- and how to export the locations to cloud-native ([Parquet](https://parquet.apache.org/) & [GeoParquet](https://geoparquet.org/)) and standard tabular (CSV) formats.\n",
    "\n",
    "Parquet and GeoParquet enable maximum interoperability with data science workflows and the modern stack.\n",
    "\n",
    "## Preconstructed Data\n",
    "If you dont want to generate your own locations, you can also download the final exported location datasets at the following links:\n",
    "- world_cereals_scaled_final_all.parquet (101 MB)\n",
    "- world_cereals_scaled_final_all.geoparquet (100 MB)\n",
    "- world_cereals_scaled_final_3m.csv (a 3M point randomly selected subset of the above tables) (379 MB)\n",
    "\n",
    "For each farm location, the final datasets include (and thus can be filtered by) the following information:\n",
    "- Crop\n",
    "- Country (Admin 0)\n",
    "- State (Admin 1)\t\n",
    "- Region (World Bank)\t\n",
    "- Subregion\t\n",
    "- Continent\t\n",
    "- aez_id\n",
    "\n",
    "## About WorldCereal\n",
    "The [ESA WorldCereal](https://www.esa.int/Applications/Observing_the_Earth/Introducing_World_Cereal) dataset consists of 10m resolution global crop type maps (classification masks) covering maize, winter cereals and spring cereals. For maize and winter cereals, there are 106 .tif (raster) files corresponding to the 106 Agricultural Ecological Zones (AEZs) across the globe. Spring cereals are only covered for 21 of the 106 AEZs. Find the article (Van Tricht et al., 2023, in pre-print) describing the full methodology [here](https://essd.copernicus.org/preprints/essd-2023-184/).\n",
    "\n",
    "For this tutorial, we used the full datasets available [here](https://zenodo.org/record/7875105). To run this notebook, download the following, unzip them, and move the folders to a folder called 'data' wherever the notebook is saved:\n",
    "- WorldCereal_2021_tc-maize-main_maize_classification.zip (18.2 GB)\n",
    "- WorldCereal_2021_tc-wintercereals_wintercereals_classification.zip (18.5 GB)\n",
    "- WorldCereal_2021_tc-springcereals_springcereals_classification.zip (7.0 GB)\n",
    "- WorldCereal_AEZ.geojson (762 KB)\n",
    "\n",
    "The ESA WorldCereal dataset may also be accessed (for smaller, user-specified regions only) through the [OpenEO API](https://github.com/Open-EO/openeo-community-examples/blob/main/python/WorldCereal/WorldCereal.ipynb).\n",
    "\n",
    "#### Why is having farm point locations a game changer?\n",
    "Data science workflows, and indeed the entire modern data stack (including cloud infrastructure more generally and cloud data warehouses), have been optimized for both storing and processing columnar data formats (tabular data, such as timeseries information). Having farm point locations (as opposed to being stored in scientific raster data like the .tif format these masks come in) means that we can begin to take advantage of the advancements that have been made in these areas, bringing digital agronomy one step closer to the modern stack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for location generation\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Required for location tagging\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "The .tif raster files are very large so we first define a function to read them in in chunks to avoid any memory-related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_in_chunks(dataset):\n",
    "    for ji, window in dataset.block_windows(1):\n",
    "        band_window = dataset.read(window=window, indexes=1)\n",
    "        yield band_window, ji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo vs Full Run\n",
    "\n",
    "In demo-mode, running the tutorial notebook will generate Maize farm locations for 3 of the 106 AEZs (covering Mexico, Guatemala, El Salvador, Honduras, and North-Western Nicaragua). It is expected to take <10min to run for these AEZs. If you wish to run the code on different AEZs covering different regions than specified, modify the aez_id_list (but note that it may take >10min). \n",
    "\n",
    "In full-mode, running the tutorial notebook will generate farm locations for Maize, Winter Cereals, and Spring Cereals for all 106 AEZs (where available). *Note running the notebook in full-mode can take numerous hours (10-15+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if the run is a demo\n",
    "demo_mode = True  # Change to False for a full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Spring Cereals (data/WorldCereal_2021_tc-springcereals_springcereals_classification)\n",
      "Processing aez_id: 15081\n",
      "Processing aez_id: 19093\n",
      "Processing aez_id: 33117\n"
     ]
    }
   ],
   "source": [
    "# Read the geojson containing the aez footprints as polygons\n",
    "aez = gpd.read_file('data/WorldCereal_AEZ.geojson')\n",
    "\n",
    "# List of all aez_ids\n",
    "aez_ids = aez['aez_id'].unique()\n",
    "\n",
    "# Create a list to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Define the directories and crop_types based on whether it is a demo run or not\n",
    "if demo_mode:\n",
    "    directories = ['data/WorldCereal_2021_tc-springcereals_springcereals_classification']\n",
    "    crop_types = {'data/WorldCereal_2021_tc-springcereals_springcereals_classification': 'Spring Cereals'}\n",
    "else:\n",
    "    directories = [\n",
    "        'data/WorldCereal_2021_tc-maize-main_maize_classification',\n",
    "        'data/WorldCereal_2021_tc-wintercereals_wintercereals_classification',\n",
    "        'data/WorldCereal_2021_tc-springcereals_springcereals_classification'\n",
    "    ]\n",
    "    crop_types = {\n",
    "        'data/WorldCereal_2021_tc-maize-main_maize_classification': 'Maize',\n",
    "        'data/WorldCereal_2021_tc-wintercereals_wintercereals_classification': 'Winter Cereals',\n",
    "        'data/WorldCereal_2021_tc-springcereals_springcereals_classification': 'Spring Cereals'\n",
    "    }\n",
    "\n",
    "# Determine the aez_id list for processing\n",
    "aez_id_list = [15081,19093,33117] if demo_mode else aez_ids\n",
    "\n",
    "# Iterate over all directories\n",
    "for directory in directories:\n",
    "\n",
    "    print(f\"Processing: {crop_types[directory]} ({directory})\")\n",
    "\n",
    "    # Iterate over all unique aez_ids\n",
    "    for aez_id in aez_id_list:\n",
    "\n",
    "        print(f\"Processing aez_id: {aez_id}\")\n",
    "\n",
    "        # Get list of all .tif files in the directory\n",
    "        all_files = os.listdir(directory)\n",
    "\n",
    "        for file in all_files:\n",
    "                    # Check if the file starts with the aez_id and ends with .tif\n",
    "                    if file.startswith(str(aez_id)) and file.endswith('.tif'):\n",
    "                        # Full path to the file\n",
    "                        filename = os.path.join(directory, file)\n",
    "\n",
    "                        # Open the raster\n",
    "                        dataset = rasterio.open(filename)\n",
    "\n",
    "                        # Read the band in chunks\n",
    "                        chunks = read_raster_in_chunks(dataset)\n",
    "\n",
    "                        # Process one chunk at a time\n",
    "                        for i, (band_chunk, ji) in enumerate(chunks):\n",
    "\n",
    "                            # Find the indices where band_chunk has values of 100\n",
    "                            indices = np.where(band_chunk == 100)\n",
    "                        \n",
    "                            # Determine the total number of classified pixels\n",
    "                            total_classified = len(indices[0])\n",
    "\n",
    "                            # Determine the proportion of classified pixels to sample\n",
    "                            proportion = 0.0001  # For example, to sample 0.01% of classified pixels\n",
    "\n",
    "                            # Determine the number of samples to draw\n",
    "                            num_samples = int(np.round(total_classified * proportion))\n",
    "\n",
    "                            # Skip this chunk if there are fewer than 1 pixels with a value of 100\n",
    "                            if num_samples < 1:\n",
    "                                continue\n",
    "\n",
    "                            # Randomly sample num_samples points from the classified indices \n",
    "                            sample_indices = np.random.choice(total_classified, size=num_samples, replace=False)\n",
    "\n",
    "                            # Get the row and column indices\n",
    "                            rows = indices[0][sample_indices] + ji[0]*band_chunk.shape[0]\n",
    "                            cols = indices[1][sample_indices] + ji[1]*band_chunk.shape[1]\n",
    "\n",
    "                            # Transform the row and column indices to the original CRS (EPSG:4326)\n",
    "                            x_coords, y_coords = dataset.xy(rows, cols)\n",
    "\n",
    "                            data = {'lon': x_coords, 'lat': y_coords, 'aez_id': aez_id, 'crop': crop_types[directory]}\n",
    "\n",
    "                            df = pd.DataFrame(data)\n",
    "\n",
    "                            # Append the df to dfs\n",
    "                            dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes in dfs\n",
    "final_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Administrative Regions\n",
    "\n",
    "We're now going to use the Natural Earth Features (https://www.naturalearthdata.com/features/) country and state-level administrative datasets to tag the farm locations generated above. For simplicity, we will access the datasets through cartopy, but the full list of datasets may also be accessed here: https://github.com/nvkelso/natural-earth-vector.\n",
    "\n",
    "*Note: you can use any boundary file that may be relevant to your use case to tag the locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shpfilename = the path to your shapefile\n",
    "ne_level_0 = shpreader.natural_earth(resolution='10m', category='cultural', name='admin_0_countries')\n",
    "ne_level_1 = shpreader.natural_earth(resolution='10m', category='cultural', name='admin_1_states_provinces')\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "countries_gdf = gpd.read_file(ne_level_0)\n",
    "states_gdf = gpd.read_file(ne_level_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries (Admin Level 0), Regions, Subregions, Contintents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the df DataFrame to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(final_df['lon'], final_df['lat'])]\n",
    "merged_geo = gpd.GeoDataFrame(final_df, geometry=geometry)\n",
    "\n",
    "# Set the CRS of the merged_geo GeoDataFrame to EPSG:4326 (WGS 84)\n",
    "merged_geo.crs = \"EPSG:4326\"\n",
    "\n",
    "# Transform the CRS of merged_geo to match the CRS of county_boundaries\n",
    "merged_geo = merged_geo.to_crs(countries_gdf.crs)\n",
    "\n",
    "# Perform a spatial join\n",
    "joined = gpd.sjoin(merged_geo, countries_gdf, predicate='within', how='left')\n",
    "\n",
    "# Remove duplicate index after spatial join\n",
    "joined = joined.loc[~joined.index.duplicated(keep='first')]\n",
    "\n",
    "# # Copy the required columns back to the df DataFrame\n",
    "final_df['Country (Admin 0)'] = joined['ADMIN']\n",
    "final_df['Region (World Bank)'] = joined['REGION_WB']\n",
    "final_df['Subregion'] = joined['SUBREGION']\n",
    "final_df['Continent'] = joined['CONTINENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States (Admin Level 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the df DataFrame to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(final_df['lon'], final_df['lat'])]\n",
    "merged_geo = gpd.GeoDataFrame(final_df, geometry=geometry)\n",
    "\n",
    "# Set the CRS of the merged_geo GeoDataFrame to EPSG:4326 (WGS 84)\n",
    "merged_geo.crs = \"EPSG:4326\"\n",
    "\n",
    "# Transform the CRS of merged_geo to match the CRS of county_boundaries\n",
    "merged_geo = merged_geo.to_crs(states_gdf.crs)\n",
    "\n",
    "# Perform a spatial join\n",
    "joined = gpd.sjoin(merged_geo, states_gdf, predicate='within', how='left')\n",
    "\n",
    "# Remove duplicate index after spatial join\n",
    "joined = joined.loc[~joined.index.duplicated(keep='first')]\n",
    "\n",
    "# # Copy the required columns back to the df DataFrame\n",
    "final_df['State (Admin 1)'] = joined['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated points caused by AEZ overlap\n",
    "\n",
    "As a result of the point generation methodology, there will be double the number of locations in regions where the Agricultural Ecological Zones overlap. We can take care of this easily by randomly filtering half of the points that fall in these regions (in reality we used 40% as the results looked better). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(final_df['lon'], final_df['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(final_df, geometry=geometry)\n",
    "\n",
    "# Set CRS for geo_df to match aez\n",
    "geo_df.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Read the geojson to a GeoDataFrame\n",
    "aez = gpd.read_file('data/WorldCereal_AEZ.geojson')\n",
    "\n",
    "# Create a spatial join between the points and the polygons to determine which points fall in overlapping polygons\n",
    "overlaps = gpd.sjoin(geo_df, aez, how='inner', predicate='within')\n",
    "\n",
    "# Group points by 'aez_id' and find the groups that have more than one polygon (overlapping areas)\n",
    "overlapping_points = overlaps.groupby(['lon', 'lat']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Sample 40% of the rows (for some reason 40% works better than 50%)\n",
    "overlapping_points_sampled = overlapping_points.sample(frac=0.40)\n",
    "\n",
    "# If you want to reset index\n",
    "overlapping_points_sampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a key in both dataframes for merging\n",
    "overlapping_points_sampled['key'] = overlapping_points_sampled['lon'].astype(str) + '-' + overlapping_points_sampled['lat'].astype(str)\n",
    "final_df['key'] = final_df['lon'].astype(str) + '-' + final_df['lat'].astype(str)\n",
    "\n",
    "# Perform a left merge to identify rows in final_df that are not present in overlapping_points_sampled\n",
    "merged_df = pd.merge(final_df, overlapping_points_sampled[['key']], on='key', how='left', indicator=True)\n",
    "\n",
    "# Filter rows that are only present in final_df (not present in overlapping_points_sampled)\n",
    "final_df_filtered = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Remove the key and _merge columns\n",
    "final_df_filtered = final_df_filtered.drop(columns=['key', '_merge'])\n",
    "\n",
    "final_df_filtered = final_df_filtered[final_df_filtered['aez_id'] != 2008]\n",
    "final_df_filtered=final_df_filtered.reset_index()\n",
    "final_df_filtered=final_df_filtered.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Out Known Problem Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_areas = [{\"type\":\"Polygon\",\"coordinates\":[[[-91.41645262581089,54.84799699573268],[-91.41645262581089,54.40865331122363],[-88.50502911906254,54.40865331122363],[-88.50502911906254,54.84799699573268],[-91.41645262581089,54.84799699573268]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-79.38191682251309,54.670532808406904],[-79.38191682251309,54.49882652204195],[-78.23689388953464,54.49882652204195],[-78.23689388953464,54.670532808406904],[-79.38191682251309,54.670532808406904]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-113.05802983596446,14.41831703002663],[-113.05802983596446,10.575178190653537],[-111.7893818359246,10.575178190653537],[-111.7893818359246,14.41831703002663],[-113.05802983596446,14.41831703002663]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-63.27121541570759,4.99991096599964],[-63.27121541570759,4.644926639828461],[-61.255111317670405,4.644926639828461],[-61.255111317670405,4.99991096599964],[-63.27121541570759,4.99991096599964]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-55.22828268903994,4.8967529432137775],[-55.22828268903994,4.6843848378761095],[-54.05519296541787,4.6843848378761095],[-54.05519296541787,4.8967529432137775],[-55.22828268903994,4.8967529432137775]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[14.734498345968422,16.466073725257086],[14.734498345968422,16.153617663042045],[16.160038004412126,16.153617663042045],[16.160038004412126,16.466073725257086],[14.734498345968422,16.466073725257086]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[10.047319891400315,25.881031010977033],[13.892714535598202,25.881031010977033],[13.892714535598202,25.90303910294598],[10.047319891400315,25.90303910294598],[10.047319891400315,25.881031010977033]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[25.107042348440654,16.476830796786135],[25.107042348440654,16.16039393217118],[29.22283561548171,16.16039393217118],[29.22283561548171,16.476830796786135],[25.107042348440654,16.476830796786135]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-4.002359251877331,3.201968162153812],[-4.002359251877331,0.9911969039241092],[-3.0296770780975124,0.9911969039241092],[-3.0296770780975124,3.201968162153812],[-4.002359251877331,3.201968162153812]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[31.16567670234616,6.839946329744742],[31.16567670234616,6.597515189353094],[32.33595020343498,6.597515189353094],[32.33595020343498,6.839946329744742],[31.16567670234616,6.839946329744742]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[40.89590023680697,19.097407436266657],[38.494290401310344,19.097407436266657],[38.494290401310344,19.368491481354894],[40.89590023680697,19.368491481354894],[40.89590023680697,19.097407436266657]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[32.93416524828737,31.775717980082778],[32.93416524828737,31.40013168824121],[33.136596368919584,31.40013168824121],[33.136596368919584,31.775717980082778],[32.93416524828737,31.775717980082778]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[85.98995391688953,39.165321115823744],[85.98995391688953,39.04406645824486],[91.67097903032186,39.04406645824486],[91.67097903032186,39.165321115823744],[85.98995391688953,39.165321115823744]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[57.034595092248644,59.83370125144946],[57.034595092248644,59.73558220090944],[58.68328068555554,59.73558220090944],[58.68328068555554,59.83370125144946],[57.034595092248644,59.83370125144946]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[16.901144386723477,37.11315362858931],[16.901144386723477,36.347826299638434],[17.259848338498678,36.347826299638434],[17.259848338498678,37.11315362858931],[16.901144386723477,37.11315362858931]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[-11.74332466793228,45.28783163983705],[-11.74332466793228,44.66939402069726],[-11.240611651323693,44.66939402069726],[-11.240611651323693,45.28783163983705],[-11.74332466793228,45.28783163983705]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[112.63187216718639,-34.99965785029128],[112.63187216718639,-35.981599748501544],[113.46733203935531,-35.981599748501544],[113.46733203935531,-34.99965785029128],[112.63187216718639,-34.99965785029128]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[73.69232615055289,15.376372183850695],[73.66955721328479,15.376372183850695],[73.66955721328479,15.715384183383437],[73.69232615055289,15.715384183383437],[73.69232615055289,15.376372183850695]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[73.58561245202628,15.226695719650461],[73.58561245202628,14.196424425273419],[73.80691834331373,14.196424425273419],[73.80691834331373,15.226695719650461],[73.58561245202628,15.226695719650461]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[121.77275981253091,53.3317057483241],[121.77275981253091,53.29519565546789],[124.27015494296099,53.29519565546789],[124.27015494296099,53.3317057483241],[121.77275981253091,53.3317057483241]]]},\n",
    "{\"type\":\"Polygon\",\"coordinates\":[[[100.19693890959938,40.29125609428906],[100.19693890959938,40.21829955885904],[104.88706903084042,40.21829955885904],[104.88706903084042,40.29125609428906],[100.19693890959938,40.29125609428906]]]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of Polygon objects from the geojson data\n",
    "problem_polygons = [Polygon(area['coordinates'][0]) for area in problem_areas]\n",
    "\n",
    "# Function to check if a point is in any of the problem areas\n",
    "def is_in_problem_area(row):\n",
    "    point = Point(row['lon'], row['lat'])\n",
    "    return any(polygon.contains(point) for polygon in problem_polygons)\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "final_df_filtered['is_in_problem_area'] = final_df_filtered.apply(is_in_problem_area, axis=1)\n",
    "\n",
    "# Filter the dataframe to remove points in problem areas\n",
    "final_df_filtered = final_df_filtered[final_df_filtered['is_in_problem_area'] == False]\n",
    "\n",
    "# You can drop the 'is_in_problem_area' column afterwards if you wish\n",
    "final_df_filtered = final_df_filtered.drop(columns=['is_in_problem_area'])\n",
    "\n",
    "final_df_filtered = final_df_filtered.reset_index()\n",
    "final_df_filtered = final_df_filtered.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Processing\n",
    "\n",
    "Reorder columns & capitalize column label text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered = final_df_filtered[['lon', 'lat', 'crop', 'Country (Admin 0)', 'State (Admin 1)', 'Region (World Bank)', 'Subregion', 'Continent', 'aez_id']]\n",
    "final_df_filtered = final_df_filtered.rename(columns={'crop': 'Crop'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Final Locations Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet ([.parquet](https://parquet.apache.org/)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered.to_parquet('world_cereals_scaled_final_all_demo.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GeoParquet ([.geoparquet](https://geoparquet.org/)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the geometry column from the coordinates\n",
    "geometry = [Point(xy) for xy in zip(final_df_filtered['lon'], final_df_filtered['lat'])]\n",
    "\n",
    "# Creating a GeoDataFrame by adding the geometry column to the original DataFrame\n",
    "final_gdf = gpd.GeoDataFrame(final_df_filtered, geometry=geometry)\n",
    "\n",
    "final_gdf.drop(columns=['lon','lat']).to_parquet('world_cereals_scaled_final_all_demo.geoparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered.to_csv('world_cereals_scaled_final_all_demo.csv') \n",
    "# final_df_filtered.sample(n=3000000, random_state=1).to_csv('world_cereals_scaled_final_3M.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
